{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "injured-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "voluntary-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-consideration",
   "metadata": {},
   "source": [
    "## Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-found",
   "metadata": {},
   "source": [
    "#### Getting a list with videos\n",
    "\n",
    "List of videos using the YouTube Data API [YouTube Data API](https://tools.digitalmethods.net/netvizz/youtube/mod_videos_list.php)\n",
    "\n",
    "Querying for the terms: `Global warming`, `Climate change`, `Paris agreement`, `Climate realism`.\n",
    "\n",
    "#### Getting all comments (including replies) to all videos in the list\n",
    "\n",
    "Get all comments to a video using the [CommentThreads method of YouTube Developer API](https://developers.google.com/youtube/v3/docs/commentThreads/list)\n",
    "\n",
    "The API documentation of CommentsThread states that it might not contain all replies: \n",
    "\n",
    ">A commentThread resource contains information about a YouTube comment thread, which comprises a top-level comment and replies, if any exist, to that comment. A commentThread resource can represent comments about either a video or a channel.\n",
    "\n",
    ">Both the top-level comment and the replies are actually comment resources nested inside the commentThread resource. The commentThread resource does not necessarily contain all replies to a comment, and you need to use the comments.list method if you want to retrieve all replies for a particular comment. Also note that some comments do not have replies.\n",
    "\n",
    "Therefore we use the [Coments list method](https://developers.google.com/youtube/v3/docs/commentThreads/list) to get all replies to a comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accepted-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyBvNFnt0KmQUBYsPc-vPAfqkkUjuvZa3CI'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-camera",
   "metadata": {},
   "source": [
    "#### List of vidoes containing the term `Paris agreement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "square-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'videolist_search50_2021_02_08-14_19_10.tab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "completed-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_videos(data_path, min_comments_count = 3):\n",
    "    videos = pd.read_csv(data_path, sep='\\t',header=(0))\n",
    "    #remove entries where commentCount is None\n",
    "    videos = videos.dropna(how='all', subset=['commentCount'])\n",
    "    #remove videos where comments count is lesser then minimum\n",
    "    videos.drop(videos[videos['commentCount'] < min_comments_count].index, inplace = True)\n",
    "    videos = videos.sort_values(['commentCount'], ascending=[False])  \n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "earlier-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = load_videos(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-latvia",
   "metadata": {},
   "source": [
    "We need to find more videos with more than a minimum number of comments. E.g. 2 comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alternative-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have in total 44624.0 from comments distributed in 47 videos containing the term Paris Agreement.\n",
      "Mean comment count: 949.4468085106383 Median: 134.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have in total {np.sum(videos['commentCount'])} from comments distributed in {len(videos)} videos containing the term Paris Agreement.\")\n",
    "print(f\"Mean comment count: {np.mean(videos['commentCount'])} Median: {np.median(videos['commentCount'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "entitled-albert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>channelId</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>videoId</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>publishedAtSQL</th>\n",
       "      <th>videoTitle</th>\n",
       "      <th>videoDescription</th>\n",
       "      <th>tags</th>\n",
       "      <th>videoCategoryId</th>\n",
       "      <th>...</th>\n",
       "      <th>dimension</th>\n",
       "      <th>definition</th>\n",
       "      <th>caption</th>\n",
       "      <th>thumbnail_maxres</th>\n",
       "      <th>licensedContent</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>dislikeCount</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>commentCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>UC3XTzVzaHQEd30rQbuvCtTQ</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>5scez5dqtAc</td>\n",
       "      <td>2017-06-05T06:30:00Z</td>\n",
       "      <td>2017-06-05 06:30:00</td>\n",
       "      <td>Paris Agreement: Last Week Tonight with John O...</td>\n",
       "      <td>Donald Trump plans to withdraw the United Stat...</td>\n",
       "      <td>last week tonight paris agreement,paris accord...</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>https://i.ytimg.com/vi/5scez5dqtAc/maxresdefau...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13068762</td>\n",
       "      <td>177317</td>\n",
       "      <td>12663</td>\n",
       "      <td>0</td>\n",
       "      <td>13779.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>UC2LZO6swZ9SLUEOks3WnsfA</td>\n",
       "      <td>2veritasium</td>\n",
       "      <td>1WKoj-kodBw</td>\n",
       "      <td>2017-06-02T21:17:43Z</td>\n",
       "      <td>2017-06-02 21:17:43</td>\n",
       "      <td>5 Bad Reasons to Ditch the Paris Climate Agree...</td>\n",
       "      <td>I've heard a lot of reasons for withdrawing fr...</td>\n",
       "      <td>veritasium,paris,donald trump,trump,climate ch...</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>https://i.ytimg.com/vi/1WKoj-kodBw/maxresdefau...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>637029</td>\n",
       "      <td>36793</td>\n",
       "      <td>5448</td>\n",
       "      <td>0</td>\n",
       "      <td>6309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>UCGaVdbSav8xWuFWTadK6loA</td>\n",
       "      <td>vlogbrothers</td>\n",
       "      <td>Sr2J_1J9w3A</td>\n",
       "      <td>2017-06-02T18:18:15Z</td>\n",
       "      <td>2017-06-02 18:18:15</td>\n",
       "      <td>The Paris Accord: What is it? And What Does it...</td>\n",
       "      <td>At the heart of the desire to get America out ...</td>\n",
       "      <td>climate change,paris agreement,paris accord,pa...</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>https://i.ytimg.com/vi/Sr2J_1J9w3A/maxresdefau...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>813125</td>\n",
       "      <td>42687</td>\n",
       "      <td>3222</td>\n",
       "      <td>0</td>\n",
       "      <td>4280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>UCZWlSUNDvCCS1hBiXV0zKcA</td>\n",
       "      <td>PragerU</td>\n",
       "      <td>47bNzLj5E_Q</td>\n",
       "      <td>2017-01-16T10:38:20Z</td>\n",
       "      <td>2017-01-16 10:38:20</td>\n",
       "      <td>The Paris Climate Agreement Won't Change the C...</td>\n",
       "      <td>The Paris Climate Agreement will cost at least...</td>\n",
       "      <td>Dennis Prager,PragerU,Prager University,Bjorn ...</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>https://i.ytimg.com/vi/47bNzLj5E_Q/maxresdefau...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3469006</td>\n",
       "      <td>20802</td>\n",
       "      <td>7839</td>\n",
       "      <td>0</td>\n",
       "      <td>4192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>UCjo1uN-aM3rmBV46xj7l2KA</td>\n",
       "      <td>John Stossel</td>\n",
       "      <td>cVkAsPizAbU</td>\n",
       "      <td>2018-03-19T10:00:30Z</td>\n",
       "      <td>2018-03-19 10:00:30</td>\n",
       "      <td>The Paris Climate Fraud</td>\n",
       "      <td>President Trump is right to withdraw from the ...</td>\n",
       "      <td>Paris climate accord,Paris Climate Agreement,P...</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>https://i.ytimg.com/vi/cVkAsPizAbU/maxresdefau...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>853178</td>\n",
       "      <td>33702</td>\n",
       "      <td>1366</td>\n",
       "      <td>0</td>\n",
       "      <td>3640.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    position                 channelId     channelTitle      videoId  \\\n",
       "1          2  UC3XTzVzaHQEd30rQbuvCtTQ  LastWeekTonight  5scez5dqtAc   \n",
       "14        15  UC2LZO6swZ9SLUEOks3WnsfA      2veritasium  1WKoj-kodBw   \n",
       "12        13  UCGaVdbSav8xWuFWTadK6loA     vlogbrothers  Sr2J_1J9w3A   \n",
       "5          6  UCZWlSUNDvCCS1hBiXV0zKcA          PragerU  47bNzLj5E_Q   \n",
       "16        17  UCjo1uN-aM3rmBV46xj7l2KA     John Stossel  cVkAsPizAbU   \n",
       "\n",
       "             publishedAt       publishedAtSQL  \\\n",
       "1   2017-06-05T06:30:00Z  2017-06-05 06:30:00   \n",
       "14  2017-06-02T21:17:43Z  2017-06-02 21:17:43   \n",
       "12  2017-06-02T18:18:15Z  2017-06-02 18:18:15   \n",
       "5   2017-01-16T10:38:20Z  2017-01-16 10:38:20   \n",
       "16  2018-03-19T10:00:30Z  2018-03-19 10:00:30   \n",
       "\n",
       "                                           videoTitle  \\\n",
       "1   Paris Agreement: Last Week Tonight with John O...   \n",
       "14  5 Bad Reasons to Ditch the Paris Climate Agree...   \n",
       "12  The Paris Accord: What is it? And What Does it...   \n",
       "5   The Paris Climate Agreement Won't Change the C...   \n",
       "16                            The Paris Climate Fraud   \n",
       "\n",
       "                                     videoDescription  \\\n",
       "1   Donald Trump plans to withdraw the United Stat...   \n",
       "14  I've heard a lot of reasons for withdrawing fr...   \n",
       "12  At the heart of the desire to get America out ...   \n",
       "5   The Paris Climate Agreement will cost at least...   \n",
       "16  President Trump is right to withdraw from the ...   \n",
       "\n",
       "                                                 tags  videoCategoryId  ...  \\\n",
       "1   last week tonight paris agreement,paris accord...               24  ...   \n",
       "14  veritasium,paris,donald trump,trump,climate ch...               27  ...   \n",
       "12  climate change,paris agreement,paris accord,pa...               22  ...   \n",
       "5   Dennis Prager,PragerU,Prager University,Bjorn ...               27  ...   \n",
       "16  Paris climate accord,Paris Climate Agreement,P...               25  ...   \n",
       "\n",
       "   dimension definition  caption  \\\n",
       "1         2d         hd    False   \n",
       "14        2d         hd     True   \n",
       "12        2d         hd     True   \n",
       "5         2d         hd     True   \n",
       "16        2d         hd     True   \n",
       "\n",
       "                                     thumbnail_maxres licensedContent  \\\n",
       "1   https://i.ytimg.com/vi/5scez5dqtAc/maxresdefau...             1.0   \n",
       "14  https://i.ytimg.com/vi/1WKoj-kodBw/maxresdefau...             1.0   \n",
       "12  https://i.ytimg.com/vi/Sr2J_1J9w3A/maxresdefau...             1.0   \n",
       "5   https://i.ytimg.com/vi/47bNzLj5E_Q/maxresdefau...             1.0   \n",
       "16  https://i.ytimg.com/vi/cVkAsPizAbU/maxresdefau...             1.0   \n",
       "\n",
       "    viewCount likeCount  dislikeCount  favoriteCount  commentCount  \n",
       "1    13068762    177317         12663              0       13779.0  \n",
       "14     637029     36793          5448              0        6309.0  \n",
       "12     813125     42687          3222              0        4280.0  \n",
       "5     3469006     20802          7839              0        4192.0  \n",
       "16     853178     33702          1366              0        3640.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "vietnamese-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Concat video lists remove duplicates based on videoId\n",
    "#TODO: find number of users commented multiple videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-portrait",
   "metadata": {},
   "source": [
    "#### Class to load all comments of a video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "republican-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video_comments:\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_key  = api_key\n",
    "        #self.video_id = video_id\n",
    "        self.max_results = 100     \n",
    "        comments_df = pd.DataFrame({\n",
    "                            'id':[],\n",
    "                            'threadId':[],\n",
    "                            'published_at': [], \n",
    "                            'author_name': [], \n",
    "                            'text': [],\n",
    "                            'is_reply': [],\n",
    "                            'likeCount': [],\n",
    "                            'cleaned': [],\n",
    "                            'video_id': [],\n",
    "                            'video_published_at': []}, \n",
    "                            columns = [ 'id',\n",
    "                                        'threadId',\n",
    "                                        'published_at', \n",
    "                                        'author_name', \n",
    "                                        'text', \n",
    "                                        'likeCount',\n",
    "                                        'is_reply', \n",
    "                                        'cleaned', \n",
    "                                        'video_id', \n",
    "                                        'video_published_at'])\n",
    "        self.comments_df = comments_df\n",
    "        \n",
    "    '''load all replies of top level comments and append dataframe witth all top level comments and replies. \n",
    "    (appendingt to df and loading replies should be devided to different methods.)'''\n",
    "    def _add_to_dataframe(self, response):\n",
    "        for i, main_comment in enumerate(response['items']):\n",
    "            comment = main_comment['snippet']['topLevelComment']['snippet']\n",
    "\n",
    "            new_row = pd.Series(data={\n",
    "                                    'id':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                    'threadId':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                    'published_at':comment['publishedAt'] , \n",
    "                                    'author_name': comment['authorDisplayName'], \n",
    "                                    'text': comment['textOriginal'],\n",
    "                                    'likeCount':comment['likeCount'],\n",
    "                                    'is_reply': 0,\n",
    "                                    'video_id': comment['videoId']})\n",
    "\n",
    "            self.comments_df = self.comments_df.append(new_row, ignore_index=True)\n",
    "\n",
    "            \n",
    "            #check if the top level comment has replies. If yey then get these too and add to df\n",
    "            request_replies = requests.get(f\"https://youtube.googleapis.com/youtube/v3/comments?part=snippet&parentId={main_comment['snippet']['topLevelComment']['id']}&key={self.api_key}\")\n",
    "            response_replies = json.loads(request_replies.text)\n",
    "        \n",
    "            #if response_replies['items'] > 0 then the main comment has replies\n",
    "            if(len(response_replies['items']) > 0):\n",
    "                for i, main_reply in enumerate(response_replies['items']):      \n",
    "                    reply = main_reply['snippet']\n",
    "\n",
    "                    new_row = pd.Series(data={\n",
    "                                            'id':reply['parentId'],\n",
    "                                            'threadId':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                            'published_at':reply['publishedAt'] , \n",
    "                                            'author_name': reply['authorDisplayName'], \n",
    "                                            'text': reply['textOriginal'],\n",
    "                                            'likeCount':reply['likeCount'],\n",
    "                                            'is_reply': 1,\n",
    "                                            'video_id': comment['videoId']})\n",
    "\n",
    "                    self.comments_df = self.comments_df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    '''Load (and append comments dataframe) recursively comments from next page until there are no next page. '''\n",
    "    def _get_next_page(self, response):     \n",
    "        request1 = requests.get(f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults={self.max_results}&pageToken={str(response['nextPageToken'])}&videoId={self.video_id}&key={self.api_key}\")\n",
    "        response1 = json.loads(request1.text)\n",
    "        self._add_to_dataframe(response1)\n",
    "        \n",
    "        if ('nextPageToken' in response1.keys()):\n",
    "            self._get_next_page(response1)\n",
    "    \n",
    "    '''Start loading comments. Paginated.'''\n",
    "    def get_comments(self, video_id):  \n",
    "        \n",
    "        self.comments_df = pd.DataFrame({\n",
    "                            'id':[],\n",
    "                            'threadId':[],\n",
    "                            'published_at': [], \n",
    "                            'author_name': [], \n",
    "                            'text': [],\n",
    "                            'is_reply': [],\n",
    "                            'likeCount': [],\n",
    "                            'cleaned': [],\n",
    "                            'video_id': [],\n",
    "                            'video_published_at': []}, \n",
    "                            columns = [ 'id',\n",
    "                                        'threadId',\n",
    "                                        'published_at', \n",
    "                                        'author_name', \n",
    "                                        'text', \n",
    "                                        'likeCount',\n",
    "                                        'is_reply', \n",
    "                                        'cleaned', \n",
    "                                        'video_id', \n",
    "                                        'video_published_at'])\n",
    "        \n",
    "\n",
    "        self.video_id = video_id\n",
    "        request  = requests.get(f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults={self.max_results}&videoId={self.video_id}&key={self.api_key}\")\n",
    "        response = json.loads(request.text)     \n",
    "        #print(len(self.comments_df))\n",
    "        #print('ADDING FIRST PAGE')\n",
    "        self._add_to_dataframe(response)\n",
    "        \n",
    "        if 'nextPageToken' in response.keys():\n",
    "            self._get_next_page(response)\n",
    "        \n",
    "        return self.comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "structured-separate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video:  1  of  47  # of comments:  3640.0\n",
      "video:  1  of  47  # of comments:  3640.0    (3320, 10)\n",
      "(3320, 10)     (3320, 10)\n"
     ]
    }
   ],
   "source": [
    "download = False\n",
    "\n",
    "if(download):\n",
    "    vid_comments = Video_comments(API_KEY)\n",
    "    all_comments_df = pd.DataFrame()\n",
    "\n",
    "    totalVideoCount = videos.shape[0]\n",
    "    counter = 1\n",
    "    max_download = 10000\n",
    "\n",
    "    for i, video in videos[4:5].iterrows():\n",
    "\n",
    "        if((len(all_comments_df) + video.commentCount) < max_download):\n",
    "            #if(counter < 14 & ):\n",
    "            print('video: ',counter,' of ',totalVideoCount,' # of comments: ',video.commentCount)\n",
    "            comments_df = vid_comments.get_comments(video.videoId)\n",
    "            print('video: ',counter,' of ',totalVideoCount,' # of comments: ',video.commentCount,'  ',comments_df.shape)\n",
    "\n",
    "            all_comments_df = pd.concat([all_comments_df, comments_df], axis=0)\n",
    "            print(all_comments_df.shape,'   ',comments_df.shape)\n",
    "        counter+=1    \n",
    "        \n",
    "    all_comments_df.shape\n",
    "    \n",
    "    # saving the DataFrame as a CSV file \n",
    "    all_comments_df.to_csv('data_raw/comments/all_comments.csv', index = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-essence",
   "metadata": {},
   "source": [
    "#### Read all comments from data_raw\\comments folder into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dated-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_folder(csv_folder):\n",
    "    ''' Input is a folder with csv files; return list of data frames.'''\n",
    "    csv_folder = Path(csv_folder).absolute()\n",
    "    csv_files = [f for f in csv_folder.iterdir() if f.name.endswith('csv')]\n",
    "    # the assign() method adds a helper column\n",
    "    dfs = [\n",
    "        pd.read_csv(csv_file)for idx, csv_file in enumerate(csv_files, 1)\n",
    "    ]\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "canadian-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = read_folder('data_raw\\comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "thorough-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "alive-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    df = df.drop(['Unnamed: 0'], axis=1)\n",
    "    all_comments_df = pd.concat([all_comments_df, df]).drop_duplicates().reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "swiss-sender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27804, 10)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "private-memorabilia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                 UghiRnPt-t7jungCoAEC\n",
       "threadId                                           UghiRnPt-t7jungCoAEC\n",
       "published_at                                       2017-06-01T19:57:05Z\n",
       "author_name                                                Daniel Casey\n",
       "text                  \"I know we messed up a bit.\" \\n\\na bit\\n\\nyeah...\n",
       "likeCount                                                        2953.0\n",
       "is_reply                                                            0.0\n",
       "cleaned                                                             NaN\n",
       "video_id                                                    VR1UhhO4UYU\n",
       "video_published_at                                                  NaN\n",
       "Name: 3078, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments_df.iloc[all_comments_df.likeCount.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-example",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
