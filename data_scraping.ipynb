{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excess-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liked-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-commitment",
   "metadata": {},
   "source": [
    "## Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-louis",
   "metadata": {},
   "source": [
    "#### Getting a list with videos\n",
    "\n",
    "List of videos using the YouTube Data API [YouTube Data API](https://tools.digitalmethods.net/netvizz/youtube/mod_videos_list.php)\n",
    "\n",
    "Querying for the terms: `Global warming`, `Climate change`, `Paris agreement`, `Climate realism`.\n",
    "\n",
    "#### Getting all comments (including replies) to all videos in the list\n",
    "\n",
    "Get all comments to a video using the [CommentThreads method of YouTube Developer API](https://developers.google.com/youtube/v3/docs/commentThreads/list)\n",
    "\n",
    "The API documentation of CommentsThread states that it might not contain all replies: \n",
    "\n",
    ">A commentThread resource contains information about a YouTube comment thread, which comprises a top-level comment and replies, if any exist, to that comment. A commentThread resource can represent comments about either a video or a channel.\n",
    "\n",
    ">Both the top-level comment and the replies are actually comment resources nested inside the commentThread resource. The commentThread resource does not necessarily contain all replies to a comment, and you need to use the comments.list method if you want to retrieve all replies for a particular comment. Also note that some comments do not have replies.\n",
    "\n",
    "Therefore we use the [Coments list method](https://developers.google.com/youtube/v3/docs/commentThreads/list) to get all replies to a comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "greater-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyAGegTsA3vp5N544npMDkbfDwZuqCOjeh0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "modified-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'videolist_search500_2021_02_07-00_46_57_climate_crisis.tab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hundred-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_videos(data_path, min_comments_count = 3):\n",
    "    videos = pd.read_csv(data_path, sep='\\t',header=(0))\n",
    "    #remove entries where commentCount is None\n",
    "    videos = videos.dropna(how='all', subset=['commentCount'])\n",
    "    #remove videos where comments count is lesser then minimum\n",
    "    videos.drop(videos[videos['commentCount'] < min_comments_count].index, inplace = True)\n",
    "    videos = videos.sort_values(['commentCount'], ascending=[False])  \n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-limitation",
   "metadata": {},
   "source": [
    "#### Class to load all comments of a video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "julian-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video_comments:\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_key  = api_key\n",
    "        #self.video_id = video_id\n",
    "        self.max_results = 100     \n",
    "        self.comments_df = None\n",
    "        self.video_published_at = None\n",
    "        self.search_term = None\n",
    "        \n",
    "    '''load all replies of top level comments and append dataframe witth all top level comments and replies. \n",
    "    (appendingt to df and loading replies should be devided to different methods.)'''\n",
    "    def _add_to_dataframe(self, response):\n",
    "        for i, main_comment in enumerate(response['items']):\n",
    "            comment = main_comment['snippet']['topLevelComment']['snippet']\n",
    "\n",
    "            new_row = pd.Series(data={\n",
    "                                    'id':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                    'threadId':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                    'published_at':comment['publishedAt'] , \n",
    "                                    'author_name': comment['authorDisplayName'], \n",
    "                                    'text': comment['textOriginal'],\n",
    "                                    'likeCount':comment['likeCount'],\n",
    "                                    'replyCount':main_comment['snippet']['totalReplyCount'],\n",
    "                                    'authorChannelId':comment['authorChannelId']['value'],\n",
    "                                    'is_reply': 0,\n",
    "                                    'video_id': comment['videoId'],\n",
    "                                    'video_published_at':self.video_published_at,\n",
    "                                    'search_term':self.search_term})\n",
    "\n",
    "            self.comments_df = self.comments_df.append(new_row, ignore_index=True)\n",
    "\n",
    "            \n",
    "            #check if the top level comment has replies. If yey then get these too and add to df\n",
    "            request_replies = requests.get(f\"https://youtube.googleapis.com/youtube/v3/comments?part=snippet&parentId={main_comment['snippet']['topLevelComment']['id']}&key={self.api_key}\")\n",
    "            response_replies = json.loads(request_replies.text)\n",
    "        \n",
    "            #if response_replies['items'] > 0 then the main comment has replies\n",
    "            if(len(response_replies['items']) > 0):\n",
    "             \n",
    "                for i, main_reply in enumerate(response_replies['items']):      \n",
    "                    reply = main_reply['snippet']\n",
    "\n",
    "                    new_row = pd.Series(data={\n",
    "                                            'id':reply['parentId'],\n",
    "                                            'threadId':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                            'published_at':reply['publishedAt'] , \n",
    "                                            'author_name': reply['authorDisplayName'], \n",
    "                                            'text': reply['textOriginal'],\n",
    "                                            'likeCount':reply['likeCount'],\n",
    "                                            'replyCount': 0,\n",
    "                                            'authorChannelId':reply['authorChannelId']['value'],\n",
    "                                            'is_reply': 1,\n",
    "                                            'video_id': comment['videoId'],\n",
    "                                            'video_published_at':self.video_published_at,\n",
    "                                            'search_term':self.search_term})\n",
    "\n",
    "                    self.comments_df = self.comments_df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    '''Load (and append comments dataframe) recursively comments from next page until there are no next page. '''\n",
    "    def _get_next_page(self, response):     \n",
    "        request1 = requests.get(f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults={self.max_results}&pageToken={str(response['nextPageToken'])}&videoId={self.video_id}&key={self.api_key}\")\n",
    "        response1 = json.loads(request1.text)\n",
    "        self._add_to_dataframe(response1)\n",
    "        \n",
    "        if ('nextPageToken' in response1.keys()):\n",
    "            self._get_next_page(response1)\n",
    "    \n",
    "    '''Start loading comments. Paginated.'''\n",
    "    def get_comments(self, video_id, video_published_at, search_term):  \n",
    "        \n",
    "        self.search_term = search_term\n",
    "        self.video_published_at = video_published_at\n",
    "        self.comments_df = pd.DataFrame({\n",
    "                            'id':[],\n",
    "                            'replyCount': [],\n",
    "                            'likeCount': [],\n",
    "                            'published_at': [], \n",
    "                            'author_name': [],\n",
    "                            'text': [],\n",
    "                            'authorChannelId':[],\n",
    "                            'is_reply': [],\n",
    "                            'threadId':[],\n",
    "                            'video_id':[],\n",
    "                            'video_published_at': [],\n",
    "                            'search_term':[]}, \n",
    "                            columns = [ 'id',\n",
    "                                        'replyCount',\n",
    "                                        'likeCount',\n",
    "                                        'published_at', \n",
    "                                        'author_name',\n",
    "                                        'text',\n",
    "                                        'authorChannelId',\n",
    "                                        'is_reply',\n",
    "                                        'threadId',\n",
    "                                        'video_id',\n",
    "                                        'video_published_at',\n",
    "                                        'search_term'])\n",
    "        \n",
    "        self.video_id = video_id\n",
    "        request  = requests.get(f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults={self.max_results}&videoId={self.video_id}&key={self.api_key}\")\n",
    "        response = json.loads(request.text)     \n",
    "        #print(len(self.comments_df))\n",
    "        #print('ADDING FIRST PAGE')\n",
    "        self._add_to_dataframe(response)\n",
    "        \n",
    "        if 'nextPageToken' in response.keys():\n",
    "            self._get_next_page(response)\n",
    "        \n",
    "        self.video_published_at = None\n",
    "        self.search_term = None\n",
    "        return self.comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-assembly",
   "metadata": {},
   "source": [
    "vid_comments = Video_comments('AIzaSyBgQr5rzBrDK9Y19ZhvgmeSGuONI0bsJLg')\n",
    "comments_df  = vid_comments.get_comments('S6GodWn4XMM', 'blaa', 'blubb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-cover",
   "metadata": {},
   "source": [
    "comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "assumed-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"List with all API keys\"\"\"\n",
    "\n",
    "api_keys = np.array(['AIzaSyCo58wzF-1eZXXTvb71cUJlzBJ2a9Dt3ms',\n",
    "                     'AIzaSyBqW3hq50uYvBpOpcdni9W_kIhBpDXAMaY',\n",
    "                     'AIzaSyAr-hhLpk-wNv7rphKC-uTuQaFDVjVHOUQ',\n",
    "                     'AIzaSyAhXjMWN0ocXSmTZTzXK_ijF9FP2v-zclo',\n",
    "                     'AIzaSyBvNFnt0KmQUBYsPc-vPAfqkkUjuvZa3CI',\n",
    "                     'AIzaSyAaFXVA2qN-zrjM8uRzoPahQAKQQR5MXbM',\n",
    "                     'AIzaSyB4gcSRVmKlkjGtiob1P7qHW-Hz-n-sjXo',\n",
    "                     'AIzaSyAgLYajPAwHIBkCjGH0RKYuzJwkvAnsNks',\n",
    "                     'AIzaSyCPv6B2vZxRIbjuZn3uYH3dqG8Rcb_dSg0',\n",
    "                     'AIzaSyD-vE5Gb8vWSh9XtcbD9hEW2uCEKZDCSA8',\n",
    "                     'AIzaSyDom1TP8XmxUy65KkQekrrr-0ScSUgC2XE',\n",
    "                     'AIzaSyApi1vYveRQOM2D2UM487T8EsRMJ71F1BA',\n",
    "                     'AIzaSyDJdq6pbdqIdkQ_atIc29hAj7tye7Zv0as',\n",
    "                     'AIzaSyAGegTsA3vp5N544npMDkbfDwZuqCOjeh0',\n",
    "                     'AIzaSyBObUNQjuCFbwrbrc1-KPbueNb3N1Uawmg',\n",
    "                     'AIzaSyAjVtZXdprTpvnaTKVIErQyDaBVRuV75Rk',\n",
    "                     'AIzaSyAbumvAdqfOqWhznK83842qEZlEI5QfyuA',\n",
    "                     'AIzaSyBgQr5rzBrDK9Y19ZhvgmeSGuONI0bsJLg',\n",
    "                     'AIzaSyDJdq6pbdqIdkQ_atIc29hAj7tye7Zv0as',\n",
    "                     'AIzaSyAGegTsA3vp5N544npMDkbfDwZuqCOjeh0',\n",
    "                     'AIzaSyBObUNQjuCFbwrbrc1-KPbueNb3N1Uawmg',\n",
    "                     'AIzaSyAjVtZXdprTpvnaTKVIErQyDaBVRuV75Rk',\n",
    "                     'AIzaSyAbumvAdqfOqWhznK83842qEZlEI5QfyuA',\n",
    "                     'AIzaSyBgQr5rzBrDK9Y19ZhvgmeSGuONI0bsJLg',\n",
    "                     'AIzaSyD7jHdD_nA_f6P4yvA8iTqxaQc75h0zaQQ',\n",
    "                     'AIzaSyCKnTjEGoP_h2sPAf5Gt39c76ahl_fFfMI',\n",
    "                     'AIzaSyD_a1wQQzU5EZK035tqy3r_kY5xgl0uJx4',\n",
    "                     'AIzaSyAWJ-AkTCsKnuPR_aIJ4VgBoBvF9krCaNY',\n",
    "                     'AIzaSyBh8cVZ1s23tdut-UjV-fpeiwJ2mXHDspM',\n",
    "                     'AIzaSyAMTJJtNemBqO6TKRj-khTO9zT2uCQsJvc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hollow-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = load_videos('summery_vid_lists/2021-03-09-15-12-13_master_video_list_below_10000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "whole-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comments_csv(videolist_name, API_KEY, max_dowload):\n",
    "    \"\"\"\n",
    "    This method creates a csv files of comments by iterating through the videos in the specified videolist.\n",
    "    A Google API key needs to be provided.\n",
    "    \n",
    "    The final csv is stored at data_raw/{number videos}_videos_{number comments}_comments_{your videlist}.csv\n",
    "    \"\"\"\n",
    "    videos = load_videos('summery_vid_lists/2021-03-09-15-12-13_master_video_list_below_10000.csv')\n",
    "\n",
    "    key = 0\n",
    "    vid_comments = Video_comments(api_keys[key])\n",
    "    totalVideoCount = videos.shape[0]\n",
    "    counter = 1\n",
    "    max_download = 10000\n",
    "    all_comments_df = pd.DataFrame()\n",
    "    \n",
    "    for i, video in videos[1:len(videos)].iterrows():\n",
    "        if((len(all_comments_df) + video.commentCount) < max_download):\n",
    "            print('video: ',counter,' of ',totalVideoCount,' # of comments: ',video.commentCount)\n",
    "            comments_df = vid_comments.get_comments(video.videoId, video.publishedAt, video.search_Term)\n",
    "            all_comments_df = pd.concat([all_comments_df, comments_df], axis=0)\n",
    "            \n",
    "            #remove the downloaded video from the list\n",
    "            videos_index = videos[videos['videoId'] == video.videoId].index \n",
    "            videos.drop(videos_index, inplace = True)\n",
    "            \n",
    "            print(all_comments_df.shape,'   ',comments_df.shape)\n",
    "            counter+=1 \n",
    "            key += 1\n",
    "        elif(key < len(api_keys)): \n",
    "            '''if a new videos comments would exceed the limit with the api keys we have \n",
    "            then take the next key from the list and expand the max_download with 10000'''   \n",
    "            print(len(all_comments_df),' + ',video.commentCount,' > 10K therefore new API key')\n",
    "            vid_comments = Video_comments(api_keys[key])\n",
    "            max_download += 10000\n",
    "            \n",
    "    #store the list of remaining videos to download\n",
    "    if(len(videos) > 0):\n",
    "        videos.to_csv('summery_vid_lists/' + 'remaining_130321' + str(len(videos)) + '_videos' + '.csv', sep='\\t', index = True)\n",
    "    \n",
    "    #store the downloaded comments\n",
    "    all_comments_df.to_csv('data_raw/comments/' + str(counter) + '_videos_' + str(len(all_comments_df)) + '_comments_' + videolist_name[:-4] + '.csv', index = True) \n",
    "    return all_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "federal-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  +  9946.0  > 10K therefore new API key\n",
      "0  +  9833.0  > 10K therefore new API key\n",
      "0  +  9785.0  > 10K therefore new API key\n",
      "0  +  8697.0  > 10K therefore new API key\n",
      "video:  1  of  450  # of comments:  8639.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6fbcec37906b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdownload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_comments_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAPI_KEY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-b9f68a067a65>\u001b[0m in \u001b[0;36mcreate_comments_csv\u001b[1;34m(videolist_name, API_KEY, max_dowload)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_comments_df\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommentCount\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_download\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'video: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' of '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotalVideoCount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' # of comments: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommentCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mcomments_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvid_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_comments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideoId\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpublishedAt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch_Term\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mall_comments_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mall_comments_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomments_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-df8142aaf6f5>\u001b[0m in \u001b[0;36mget_comments\u001b[1;34m(self, video_id, video_published_at, search_term)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m#print(len(self.comments_df))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m#print('ADDING FIRST PAGE')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_to_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'nextPageToken'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-df8142aaf6f5>\u001b[0m in \u001b[0;36m_add_to_dataframe\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;31m#check if the top level comment has replies. If yey then get these too and add to df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mrequest_replies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"https://youtube.googleapis.com/youtube/v3/comments?part=snippet&parentId={main_comment['snippet']['topLevelComment']['id']}&key={self.api_key}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mresponse_replies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest_replies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m             )\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[1;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[0mtls_in_tls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             conn = connection.create_connection(\n\u001b[1;32m--> 170\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             )\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "download = True\n",
    "if(download):\n",
    "    df = create_comments_csv(data_path, API_KEY, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-pollution",
   "metadata": {},
   "source": [
    "video:  1  of  450  # of comments:  9946.0\n",
    "(8145, 12)     (8145, 12)\n",
    "video:  2  of  450  # of comments:  1853.0\n",
    "(9823, 12)     (1678, 12)\n",
    "video:  3  of  450  # of comments:  176.0\n",
    "(10000, 12)     (177, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-brick",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
