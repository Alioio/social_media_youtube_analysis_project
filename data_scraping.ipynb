{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "subtle-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "antique-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-chance",
   "metadata": {},
   "source": [
    "## Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-karaoke",
   "metadata": {},
   "source": [
    "#### Getting a list with videos\n",
    "\n",
    "List of videos using the YouTube Data API [YouTube Data API](https://tools.digitalmethods.net/netvizz/youtube/mod_videos_list.php)\n",
    "\n",
    "Querying for the terms: `Global warming`, `Climate change`, `Paris agreement`, `Climate realism`.\n",
    "\n",
    "#### Getting all comments (including replies) to all videos in the list\n",
    "\n",
    "Get all comments to a video using the [CommentThreads method of YouTube Developer API](https://developers.google.com/youtube/v3/docs/commentThreads/list)\n",
    "\n",
    "The API documentation of CommentsThread states that it might not contain all replies: \n",
    "\n",
    ">A commentThread resource contains information about a YouTube comment thread, which comprises a top-level comment and replies, if any exist, to that comment. A commentThread resource can represent comments about either a video or a channel.\n",
    "\n",
    ">Both the top-level comment and the replies are actually comment resources nested inside the commentThread resource. The commentThread resource does not necessarily contain all replies to a comment, and you need to use the comments.list method if you want to retrieve all replies for a particular comment. Also note that some comments do not have replies.\n",
    "\n",
    "Therefore we use the [Coments list method](https://developers.google.com/youtube/v3/docs/commentThreads/list) to get all replies to a comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "spatial-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyAGegTsA3vp5N544npMDkbfDwZuqCOjeh0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "diagnostic-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'videolist_search500_2021_02_07-00_46_57_climate_crisis.tab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "damaged-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_videos(data_path, min_comments_count = 3):\n",
    "    videos = pd.read_csv(data_path, sep='\\t',header=(0))\n",
    "    #remove entries where commentCount is None\n",
    "    videos = videos.dropna(how='all', subset=['commentCount'])\n",
    "    #remove videos where comments count is lesser then minimum\n",
    "    videos.drop(videos[videos['commentCount'] < min_comments_count].index, inplace = True)\n",
    "    videos = videos.sort_values(['commentCount'], ascending=[False])  \n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-stuart",
   "metadata": {},
   "source": [
    "#### Class to load all comments of a video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "chief-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video_comments:\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_key  = api_key\n",
    "        #self.video_id = video_id\n",
    "        self.max_results = 100     \n",
    "        self.comments_df = None\n",
    "        self.video_published_at = None\n",
    "        self.search_term = None\n",
    "        \n",
    "    '''load all replies of top level comments and append dataframe witth all top level comments and replies. \n",
    "    (appendingt to df and loading replies should be devided to different methods.)'''\n",
    "    def _add_to_dataframe(self, response):\n",
    "        for i, main_comment in enumerate(response['items']):\n",
    "            comment = main_comment['snippet']['topLevelComment']['snippet']\n",
    "\n",
    "            new_row = pd.Series(data={\n",
    "                                    'id':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                    'threadId':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                    'published_at':comment['publishedAt'] , \n",
    "                                    'author_name': comment['authorDisplayName'], \n",
    "                                    'text': comment['textOriginal'],\n",
    "                                    'likeCount':comment['likeCount'],\n",
    "                                    'replyCount':main_comment['snippet']['totalReplyCount'],\n",
    "                                    'authorChannelId':comment['authorChannelId']['value'],\n",
    "                                    'is_reply': 0,\n",
    "                                    'video_id': comment['videoId'],\n",
    "                                    'video_published_at':self.video_published_at,\n",
    "                                    'search_term':self.search_term})\n",
    "\n",
    "            self.comments_df = self.comments_df.append(new_row, ignore_index=True)\n",
    "\n",
    "            \n",
    "            #check if the top level comment has replies. If yey then get these too and add to df\n",
    "            request_replies = requests.get(f\"https://youtube.googleapis.com/youtube/v3/comments?part=snippet&parentId={main_comment['snippet']['topLevelComment']['id']}&key={self.api_key}\")\n",
    "            response_replies = json.loads(request_replies.text)\n",
    "        \n",
    "            #if response_replies['items'] > 0 then the main comment has replies\n",
    "            if(len(response_replies['items']) > 0):\n",
    "             \n",
    "                for i, main_reply in enumerate(response_replies['items']):      \n",
    "                    reply = main_reply['snippet']\n",
    "\n",
    "                    new_row = pd.Series(data={\n",
    "                                            'id':reply['parentId'],\n",
    "                                            'threadId':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                            'published_at':reply['publishedAt'] , \n",
    "                                            'author_name': reply['authorDisplayName'], \n",
    "                                            'text': reply['textOriginal'],\n",
    "                                            'likeCount':reply['likeCount'],\n",
    "                                            'replyCount': 0,\n",
    "                                            'authorChannelId':reply['authorChannelId']['value'],\n",
    "                                            'is_reply': 1,\n",
    "                                            'video_id': comment['videoId'],\n",
    "                                            'video_published_at':self.video_published_at,\n",
    "                                            'search_term':self.search_term})\n",
    "\n",
    "                    self.comments_df = self.comments_df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    '''Load (and append comments dataframe) recursively comments from next page until there are no next page. '''\n",
    "    def _get_next_page(self, response):     \n",
    "        request1 = requests.get(f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults={self.max_results}&pageToken={str(response['nextPageToken'])}&videoId={self.video_id}&key={self.api_key}\")\n",
    "        response1 = json.loads(request1.text)\n",
    "        self._add_to_dataframe(response1)\n",
    "        \n",
    "        if ('nextPageToken' in response1.keys()):\n",
    "            self._get_next_page(response1)\n",
    "    \n",
    "    '''Start loading comments. Paginated.'''\n",
    "    def get_comments(self, video_id, video_published_at, search_term):  \n",
    "        \n",
    "        self.search_term = search_term\n",
    "        self.video_published_at = video_published_at\n",
    "        self.comments_df = pd.DataFrame({\n",
    "                            'id':[],\n",
    "                            'replyCount': [],\n",
    "                            'likeCount': [],\n",
    "                            'published_at': [], \n",
    "                            'author_name': [],\n",
    "                            'text': [],\n",
    "                            'authorChannelId':[],\n",
    "                            'is_reply': [],\n",
    "                            'threadId':[],\n",
    "                            'video_id':[],\n",
    "                            'video_published_at': [],\n",
    "                            'search_term':[]}, \n",
    "                            columns = [ 'id',\n",
    "                                        'replyCount',\n",
    "                                        'likeCount',\n",
    "                                        'published_at', \n",
    "                                        'author_name',\n",
    "                                        'text',\n",
    "                                        'authorChannelId',\n",
    "                                        'is_reply',\n",
    "                                        'threadId',\n",
    "                                        'video_id',\n",
    "                                        'video_published_at',\n",
    "                                        'search_term'])\n",
    "        \n",
    "        self.video_id = video_id\n",
    "        request  = requests.get(f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults={self.max_results}&videoId={self.video_id}&key={self.api_key}\")\n",
    "        response = json.loads(request.text)     \n",
    "        #print(len(self.comments_df))\n",
    "        #print('ADDING FIRST PAGE')\n",
    "        self._add_to_dataframe(response)\n",
    "        \n",
    "        if 'nextPageToken' in response.keys():\n",
    "            self._get_next_page(response)\n",
    "        \n",
    "        self.video_published_at = None\n",
    "        self.search_term = None\n",
    "        return self.comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-letters",
   "metadata": {},
   "source": [
    "vid_comments = Video_comments('AIzaSyCo58wzF-1eZXXTvb71cUJlzBJ2a9Dt3ms')\n",
    "comments_df  = vid_comments.get_comments('oo5ca1dMbEc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-mirror",
   "metadata": {},
   "source": [
    "Index(['id', 'replyCount', 'likeCount', 'published_at', 'author_name', 'text',\n",
    "       'authorChannelId', 'is_reply', 'threadId', 'video_id',\n",
    "       'video_published_at'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "robust-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"List with all API keys\"\"\"\n",
    "api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "regulated-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = load_videos('summery_vid_lists/2021-03-09-15-12-13_master_video_list_below_10000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pediatric-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comments_csv(videolist_name, API_KEY, max_dowload):\n",
    "    \"\"\"\n",
    "    This method creates a csv files of comments by iterating through the videos in the specified videolist.\n",
    "    A Google API key needs to be provided.\n",
    "    \n",
    "    The final csv is stored at data_raw/{number videos}_videos_{number comments}_comments_{your videlist}.csv\n",
    "    \"\"\"\n",
    "    videos = load_videos('summery_vid_lists/2021-03-09-15-12-13_master_video_list_below_10000.csv')\n",
    "    key = 0\n",
    "    vid_comments = Video_comments(api_keys[key])\n",
    "    totalVideoCount = videos.shape[0]\n",
    "    counter = 1\n",
    "    max_download = 10000\n",
    "    all_comments_df = pd.DataFrame()\n",
    "    \n",
    "    for i, video in videos[1:len(videos)].iterrows():\n",
    "        if((len(all_comments_df) + video.commentCount) < max_download):\n",
    "            print('video: ',counter,' of ',totalVideoCount,' # of comments: ',video.commentCount)\n",
    "            comments_df = vid_comments.get_comments(video.videoId, video.publishedAt, video.search_Term)\n",
    "            all_comments_df = pd.concat([all_comments_df, comments_df], axis=0)\n",
    "            \n",
    "            #remove the downloaded video from the list\n",
    "            videos_index = videos[videos['videoId'] == video.videoId].index \n",
    "            videos.drop(videos_index, inplace = True)\n",
    "            \n",
    "            print(all_comments_df.shape,'   ',comments_df.shape)\n",
    "            counter+=1 \n",
    "        elif(key < len(api_keys)): \n",
    "            '''if a new videos comments would exceed the limit with the api keys we have \n",
    "            then take the next key from the list and expand the max_download with 10000'''   \n",
    "            key += 1\n",
    "            print(len(all_comments_df),' + ',video.commentCount,' > 10K therefore new API key')\n",
    "            vid_comments = Video_comments(api_keys[key])\n",
    "            max_download += 10000\n",
    "            counter = 1\n",
    "            \n",
    "    #store the list of remaining videos to download\n",
    "    if(len(videos) > 0):\n",
    "        videos.to_csv('summery_vid_lists/' + 'remaining_110321' + str(len(videos)) + '_videos' + '.csv', index = True)\n",
    "    \n",
    "    #store the downloaded comments\n",
    "    all_comments_df.to_csv('data_raw/comments/' + str(counter) + '_videos_' + str(len(all_comments_df)) + '_comments_' + videolist_name[:-4] + '.csv', index = True) \n",
    "    return all_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "approximate-terminal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video:  1  of  450  # of comments:  9946.0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-6fbcec37906b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdownload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_comments_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAPI_KEY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-83c125002c8b>\u001b[0m in \u001b[0;36mcreate_comments_csv\u001b[1;34m(videolist_name, API_KEY, max_dowload)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_comments_df\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommentCount\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_download\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'video: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' of '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotalVideoCount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' # of comments: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommentCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mcomments_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvid_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_comments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideoId\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpublishedAt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch_Term\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mall_comments_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mall_comments_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomments_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-df8142aaf6f5>\u001b[0m in \u001b[0;36mget_comments\u001b[1;34m(self, video_id, video_published_at, search_term)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m#print(len(self.comments_df))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m#print('ADDING FIRST PAGE')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_to_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'nextPageToken'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-df8142aaf6f5>\u001b[0m in \u001b[0;36m_add_to_dataframe\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m     12\u001b[0m     (appendingt to df and loading replies should be devided to different methods.)'''\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_to_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_comment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'items'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mcomment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain_comment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'snippet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topLevelComment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'snippet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'items'"
     ]
    }
   ],
   "source": [
    "download = True\n",
    "if(download):\n",
    "    df = create_comments_csv(data_path, API_KEY, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-humidity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
