{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting a list with videos\n",
    "\n",
    "List of videos using the YouTube Data API [YouTube Data API](https://tools.digitalmethods.net/netvizz/youtube/mod_videos_list.php)\n",
    "\n",
    "Querying for the terms: `Global warming`, `Climate change`, `Paris agreement`, `Climate realism`.\n",
    "\n",
    "#### Getting all comments (including replies) to all videos in the list\n",
    "\n",
    "Get all comments to a video using the [CommentThreads method of YouTube Developer API](https://developers.google.com/youtube/v3/docs/commentThreads/list)\n",
    "\n",
    "The API documentation of CommentsThread states that it might not contain all replies: \n",
    "\n",
    ">A commentThread resource contains information about a YouTube comment thread, which comprises a top-level comment and replies, if any exist, to that comment. A commentThread resource can represent comments about either a video or a channel.\n",
    "\n",
    ">Both the top-level comment and the replies are actually comment resources nested inside the commentThread resource. The commentThread resource does not necessarily contain all replies to a comment, and you need to use the comments.list method if you want to retrieve all replies for a particular comment. Also note that some comments do not have replies.\n",
    "\n",
    "Therefore we use the [Coments list method](https://developers.google.com/youtube/v3/docs/commentThreads/list) to get all replies to a comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyAGegTsA3vp5N544npMDkbfDwZuqCOjeh0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'videolist_search500_2021_02_07-00_46_57_climate_crisis.tab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_videos(data_path, min_comments_count = 3):\n",
    "    videos = pd.read_csv(data_path, sep='\\t',header=(0))\n",
    "    #remove entries where commentCount is None\n",
    "    videos = videos.dropna(how='all', subset=['commentCount'])\n",
    "    #remove videos where comments count is lesser then minimum\n",
    "    videos.drop(videos[videos['commentCount'] < min_comments_count].index, inplace = True)\n",
    "    videos = videos.sort_values(['commentCount'], ascending=[False])  \n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class to load all comments of a video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video_comments:\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_key  = api_key\n",
    "        #self.video_id = video_id\n",
    "        self.max_results = 100     \n",
    "        self.comments_df = None\n",
    "        self.video_published_at = None\n",
    "        self.search_term = None\n",
    "        \n",
    "    '''load all replies of top level comments and append dataframe witth all top level comments and replies. \n",
    "    (appendingt to df and loading replies should be devided to different methods.)'''\n",
    "    def _add_to_dataframe(self, response):\n",
    "        for i, main_comment in enumerate(response['items']):\n",
    "            comment = main_comment['snippet']['topLevelComment']['snippet']\n",
    "\n",
    "            new_row = pd.Series(data={\n",
    "                                    'id':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                    'threadId':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                    'published_at':comment['publishedAt'] , \n",
    "                                    'author_name': comment['authorDisplayName'], \n",
    "                                    'text': comment['textOriginal'],\n",
    "                                    'likeCount':comment['likeCount'],\n",
    "                                    'replyCount':main_comment['snippet']['totalReplyCount'],\n",
    "                                    'authorChannelId':comment['authorChannelId']['value'],\n",
    "                                    'is_reply': 0,\n",
    "                                    'video_id': comment['videoId'],\n",
    "                                    'video_published_at':self.video_published_at,\n",
    "                                    'search_term':self.search_term})\n",
    "\n",
    "            self.comments_df = self.comments_df.append(new_row, ignore_index=True)\n",
    "\n",
    "            \n",
    "            #check if the top level comment has replies. If yey then get these too and add to df\n",
    "            request_replies = requests.get(f\"https://youtube.googleapis.com/youtube/v3/comments?part=snippet&parentId={main_comment['snippet']['topLevelComment']['id']}&key={self.api_key}\")\n",
    "            response_replies = json.loads(request_replies.text)\n",
    "        \n",
    "            #if response_replies['items'] > 0 then the main comment has replies\n",
    "            if(len(response_replies['items']) > 0):\n",
    "             \n",
    "                for i, main_reply in enumerate(response_replies['items']):      \n",
    "                    reply = main_reply['snippet']\n",
    "\n",
    "                    new_row = pd.Series(data={\n",
    "                                            'id':reply['parentId'],\n",
    "                                            'threadId':main_comment['snippet']['topLevelComment']['id'],\n",
    "                                            'published_at':reply['publishedAt'] , \n",
    "                                            'author_name': reply['authorDisplayName'], \n",
    "                                            'text': reply['textOriginal'],\n",
    "                                            'likeCount':reply['likeCount'],\n",
    "                                            'replyCount': 0,\n",
    "                                            'authorChannelId':reply['authorChannelId']['value'],\n",
    "                                            'is_reply': 1,\n",
    "                                            'video_id': comment['videoId'],\n",
    "                                            'video_published_at':self.video_published_at,\n",
    "                                            'search_term':self.search_term})\n",
    "\n",
    "                    self.comments_df = self.comments_df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    '''Load (and append comments dataframe) recursively comments from next page until there are no next page. '''\n",
    "    def _get_next_page(self, response):     \n",
    "        request1 = requests.get(f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults={self.max_results}&pageToken={str(response['nextPageToken'])}&videoId={self.video_id}&key={self.api_key}\")\n",
    "        response1 = json.loads(request1.text)\n",
    "        self._add_to_dataframe(response1)\n",
    "        \n",
    "        if ('nextPageToken' in response1.keys()):\n",
    "            self._get_next_page(response1)\n",
    "    \n",
    "    '''Start loading comments. Paginated.'''\n",
    "    def get_comments(self, video_id, video_published_at, search_term):  \n",
    "        \n",
    "        self.search_term = search_term\n",
    "        self.video_published_at = video_published_at\n",
    "        self.comments_df = pd.DataFrame({\n",
    "                            'id':[],\n",
    "                            'replyCount': [],\n",
    "                            'likeCount': [],\n",
    "                            'published_at': [], \n",
    "                            'author_name': [],\n",
    "                            'text': [],\n",
    "                            'authorChannelId':[],\n",
    "                            'is_reply': [],\n",
    "                            'threadId':[],\n",
    "                            'video_id':[],\n",
    "                            'video_published_at': [],\n",
    "                            'search_term':[]}, \n",
    "                            columns = [ 'id',\n",
    "                                        'replyCount',\n",
    "                                        'likeCount',\n",
    "                                        'published_at', \n",
    "                                        'author_name',\n",
    "                                        'text',\n",
    "                                        'authorChannelId',\n",
    "                                        'is_reply',\n",
    "                                        'threadId',\n",
    "                                        'video_id',\n",
    "                                        'video_published_at',\n",
    "                                        'search_term'])\n",
    "        \n",
    "        self.video_id = video_id\n",
    "        request  = requests.get(f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults={self.max_results}&videoId={self.video_id}&key={self.api_key}\")\n",
    "        response = json.loads(request.text)     \n",
    "        #print(len(self.comments_df))\n",
    "        #print('ADDING FIRST PAGE')\n",
    "        self._add_to_dataframe(response)\n",
    "        \n",
    "        if 'nextPageToken' in response.keys():\n",
    "            self._get_next_page(response)\n",
    "        \n",
    "        self.video_published_at = None\n",
    "        self.search_term = None\n",
    "        return self.comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vid_comments = Video_comments('AIzaSyAMTJJtNemBqO6TKRj-khTO9zT2uCQsJvc')\n",
    "comments_df  = vid_comments.get_comments('wAwIR1CEqP0', 'blaa', 'blubb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"List with all API keys\"\"\"\n",
    "api_keys = np.array([\n",
    "\n",
    "                     'AIzaSyAGegTsA3vp5N544npMDkbfDwZuqCOjeh0',\n",
    "                     ])\n",
    "#'AIzaSyDJdq6pbdqIdkQ_atIc29hAj7tye7Zv0as',\n",
    "# 'AIzaSyBgQr5rzBrDK9Y19ZhvgmeSGuONI0bsJLg'\n",
    "                   #  'AIzaSyAGegTsA3vp5N544npMDkbfDwZuqCOjeh0',\n",
    "                     #'AIzaSyBObUNQjuCFbwrbrc1-KPbueNb3N1Uawmg',\n",
    "                    # 'AIzaSyAjVtZXdprTpvnaTKVIErQyDaBVRuV75Rk',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos = pd.read_csv(\"summery_vid_lists/2021-03-13-12-42-01_master_video_list_below_5000.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos[videos.Comments_Downloaded==False].iloc[:35].commentCount.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos[videos.Comments_Downloaded==False].iloc[35:120].to_csv(\"summery_vid_lists/2021-03-13-12-42-01_master_video_list_below_5000_Flo.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comments_csv(videolist_name, api_keys, max_download=10000):\n",
    "    \"\"\"\n",
    "    This method creates a csv files of comments by iterating through the videos in the specified videolist.\n",
    "    A Google API key needs to be provided.\n",
    "    \n",
    "    The final csv is stored at data_raw/{number videos}_videos_{number comments}_comments_{your videlist}.csv\n",
    "    \"\"\"\n",
    "    videos = load_videos(videolist_name)\n",
    "\n",
    "    key = 0\n",
    "    vid_comments = Video_comments(api_keys[key])\n",
    "    totalVideoCount = videos.shape[0]\n",
    "    counter = 1\n",
    "    #max_download = 10000\n",
    "    all_comments_df = pd.DataFrame()\n",
    "    \n",
    "    for i, video in videos[1:len(videos)].iterrows():   \n",
    "        #überprüfe ob Comments_Downloaded True ist. Wenn True dann nächstes Video.\n",
    "        if(video.Comments_Downloaded == False):\n",
    "            if((len(all_comments_df) + video.commentCount) < max_download):\n",
    "                \n",
    "                try:\n",
    "                    print('video: ',counter,' of ',totalVideoCount,' # of comments: ',video.commentCount)\n",
    "                    comments_df = vid_comments.get_comments(video.videoId, video.publishedAt, video.search_Term)\n",
    "                    all_comments_df = pd.concat([all_comments_df, comments_df], axis=0)\n",
    "\n",
    "                    #Setze Comments_Downloaded auf True. \n",
    "                    #video.Comments_Downloaded = True # This does not change the original dataframe\n",
    "                    videos.loc[i, \"Comments_Downloaded\"] = True\n",
    "\n",
    "                    print(all_comments_df.shape,'   ',comments_df.shape)\n",
    "                    counter+=1 \n",
    "                except Exception as e:\n",
    "                    print('Error while downloading video: ',video.videoId,'  ',repr(e))\n",
    "                    print('Currently using key ', key+1, 'of ', len(api_keys))\n",
    "\n",
    "            elif(key < (len(api_keys)-1)): \n",
    "                '''if a new videos comments would exceed the limit with the api keys we have \n",
    "                then take the next key from the list and expand the max_download with 10000''' \n",
    "                key += 1\n",
    "                print(len(all_comments_df),' + ',video.commentCount,' > 10K therefore new API key')\n",
    "                vid_comments = Video_comments(api_keys[key])\n",
    "                max_download += 10000\n",
    "                \n",
    "    #store updated list of videos\n",
    "    videos.to_csv(videolist_name, sep='\\t', index = True)\n",
    "    \n",
    "    #store the downloaded comments\n",
    "    try:\n",
    "        all_comments_df.to_csv('data_raw/comments/' + str(counter) + '_videos_' + str(len(all_comments_df)) + '_comments_' + videolist_name[-8:-4] + '.csv', index = True) \n",
    "        return all_comments_df\n",
    "    except:\n",
    "        print(\"Dataframe was not saved, but will be returned.\")\n",
    "        return all_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video:  1  of  85  # of comments:  995.0\n",
      "(979, 12)     (979, 12)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_raw/comments/2_videos_979_comments_summery_vid_lists/2021-03-13-12-42-01_master_video_list_below_5000_Flo.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a17e995a0aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_comments_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'summery_vid_lists/2021-03-13-12-42-01_master_video_list_below_5000_Flo.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-84c570975d9d>\u001b[0m in \u001b[0;36mcreate_comments_csv\u001b[0;34m(videolist_name, api_keys, max_download)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#store the downloaded comments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mall_comments_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_raw/comments/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_videos_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_comments_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_comments_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvideolist_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_comments_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_raw/comments/2_videos_979_comments_summery_vid_lists/2021-03-13-12-42-01_master_video_list_below_5000_Flo.csv'"
     ]
    }
   ],
   "source": [
    "download = True\n",
    "if(download):\n",
    "    df = create_comments_csv('summery_vid_lists/2021-03-13-12-42-01_master_video_list_below_5000_Flo.csv', api_keys, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_folder(csv_folder):\n",
    "    ''' Input is a folder with csv files; return list of data frames.'''\n",
    "    csv_folder = pathlib.Path(csv_folder).absolute()\n",
    "    csv_files = [f for f in csv_folder.iterdir() if f.name.endswith('csv')]\n",
    "    # the assign() method adds a helper column\n",
    "    dfs = [\n",
    "        pd.read_csv(csv_file)for idx, csv_file in enumerate(csv_files, 1)\n",
    "    ]\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_folder(csv_folder):\n",
    "    ''' Input is a folder with csv files; return list of data frames.'''\n",
    "    csv_folder = Path(csv_folder).absolute()\n",
    "    csv_files = [f for f in csv_folder.iterdir() if f.name.endswith('csv')]\n",
    "    # the assign() method adds a helper column\n",
    "    dfs = [\n",
    "        pd.read_csv(csv_file)for idx, csv_file in enumerate(csv_files, 1)\n",
    "    ]\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csv_files(folder_name='data_raw\\comments'):\n",
    "    dfs = read_folder(folder_name)\n",
    "    all_comments_df = pd.DataFrame()\n",
    "    for df in dfs:\n",
    "        df = df.drop(['Unnamed: 0'], axis=1)\n",
    "        all_comments_df = pd.concat([all_comments_df, df]).drop_duplicates().reset_index(drop=True) \n",
    "    return all_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "all_df = concat_csv_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_csv('data_raw/comments/concatenated_date_140321_videos_' + str(len(np.unique(all_df.video_id))) + '_comments_' + str(all_df.shape[0]) + '.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
